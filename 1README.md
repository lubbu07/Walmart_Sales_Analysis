
# ğŸ› Walmart Sales Analysis | SQL + Python End-to-End Project

## ğŸ“Œ Overview

![Project Pipeline](https://github.com/najirh/Walmart_SQL_Python/blob/main/walmart_project-piplelines.png)

This project is a hands-on implementation of a complete data analysis workflow on Walmart sales data. Using a combination of SQL and Python, I extracted, cleaned, analyzed, and loaded data to solve real-world business problems. The project emphasizes practical skills in data manipulation, querying relational databases, and delivering actionable insights for decision-making.

---

## ğŸ› ï¸ Project Workflow

### 1. Environment Setup
- **Tools & Technologies**: Python, MySQL, PostgreSQL, VS Code
- **Objective**: Establish an organized workspace and prepare the local environment for efficient data processing and development.

### 2. Kaggle API Configuration
- **Setup**: Generated and downloaded a Kaggle API token from [Kaggle](https://www.kaggle.com/).
- **Integration Steps**:
  - Stored the `kaggle.json` credentials file securely under `.kaggle/`.
  - Used `kaggle datasets download -d <dataset-name>` to fetch the dataset programmatically.

### 3. Dataset Acquisition
- **Source**: Walmart sales data hosted on Kaggle.
- **Dataset URL**: [Walmart Sales Dataset](https://www.kaggle.com/najir0123/walmart-10k-sales-datasets)
- **Storage Location**: Saved in the `data/` directory for streamlined access.

### 4. Library Installation & Data Loading
- **Required Libraries**:
  ```bash
  pip install pandas numpy sqlalchemy mysql-connector-python psycopg2
  ```
- **Data Import**: Loaded CSV files into Pandas DataFrames to perform initial inspection and preparation.

### 5. Initial Data Exploration
- **Purpose**: Understand column structure, detect anomalies, and identify transformations needed.
- **Methods Used**: `.info()`, `.head()`, `.describe()`, column type checking.

### 6. Data Cleaning Process
- Removed unnecessary duplicates to ensure accuracy.
- Treated missing or null entries through appropriate imputation or removal.
- Ensured consistent data typesâ€”converted strings to `datetime`, formatted currency fields, etc.
- Verified overall data integrity after transformations.

### 7. Feature Engineering
- Added a derived column `Total Amount` using:  
  `unit_price Ã— quantity`
- These transformations improved downstream aggregation and analysis in SQL queries.

### 8. Database Integration
- **Connection Setup**: Used SQLAlchemy to connect Python to both MySQL and PostgreSQL.
- **Table Creation & Insertion**: Defined schema and loaded processed data into database tables.
- **Validation**: Ran sample queries to confirm data insertion and structure accuracy.

---

## ğŸ“Š Business Questions Solved

I designed and executed SQL queries to answer the following business-critical questions:

- ğŸ“Œ What are the revenue trends by store branch and product category?
- ğŸ›’ Which categories and items are the best performers?
- ğŸ“ How does sales performance vary by city, weekday, and time?
- â° What are the peak shopping periods during the week or day?
- ğŸ’° How do profit margins differ between locations and product types?
- ğŸ’³ What are the most preferred payment methods among customers?

All queries were logged, and results interpreted to provide insights for operations and strategy.

---

## ğŸ’» Technologies Used

| Category      | Tools & Libraries                                  |
|---------------|----------------------------------------------------|
| Programming   | Python (Pandas, NumPy)                             |
| Databases     | MySQL, PostgreSQL                                  |
| Data Transfer | SQLAlchemy, MySQL Connector, psycopg2              |
| Other Tools   | Kaggle API, VS Code, Git, GitHub                   |

---

## ğŸ“ Project Structure

```
walmart-sales-analysis/
â”‚
â”œâ”€â”€ data/              # Raw and cleaned datasets
â”œâ”€â”€ sql_queries/       # SQL scripts for business questions
â”œâ”€â”€ notebooks/         # Jupyter notebooks for EDA and Python work
â”œâ”€â”€ main.py            # Python script to clean and load data
â”œâ”€â”€ requirements.txt   # List of Python dependencies
â””â”€â”€ README.md          # Project documentation
```

---

## ğŸ“ˆ Key Insights

- ğŸ¬ Branch C generated the highest revenue, while Branch A had the best profit margins.
- ğŸ‘• The Fashion category had the highest number of items sold across all cities.
- ğŸ’³ E-wallet was the fastest-growing payment method, especially on weekends.
- ğŸ•’ Evening hours and Fridays recorded peak sales.
- ğŸŒ† Customer ratings were consistently higher in urban branches.

---

## ğŸ”® Future Work

- Build an interactive dashboard using Power BI or Tableau.
- Expand analysis with external datasets such as regional income, footfall, or customer demographics.
- Automate daily ETL pipelines for real-time insights and alerts.

---

## âœ… Requirements

- Python 3.8 or above
- SQL Database (MySQL/PostgreSQL)
- Python Libraries:
  - `pandas`, `numpy`, `sqlalchemy`, `mysql-connector-python`, `psycopg2`
- Kaggle account and API token for dataset access

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Getting Started

1. Clone the repository:
   ```bash
   git clone <repo-url>
   ```
2. Set up Kaggle API and download the dataset.
3. Run `main.py` to clean and load the data.
4. Explore SQL business queries in the `sql_queries/` folder.

---

## ğŸ“„ License

Licensed under the MIT License.

---

## ğŸ™ Acknowledgments

- Data Source - Dataset provided by Kaggle.
- Inspiration - Inspired by real-world Walmartâ€™s business case studies on sales and supply chain optimization.

---
